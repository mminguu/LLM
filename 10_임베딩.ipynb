{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9550e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# êµì¬ëŠ” tensorflowë¡œ ë˜ì–´ ìˆì§€ë§Œ ìš°ë¦¬ëŠ” íŒŒì´í† ì¹˜ë¥¼ ì‚¬ìš©.\n",
    "# ë³µìŠµë–„ íƒ ì„œí”Œë¡œìš° ì‚¬ìš©í•´ë„ ë¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50953a1",
   "metadata": {},
   "source": [
    "### ğŸ§  RNN (Recurrent Neural Network) ìš”ì•½ì •ë¦¬\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“Œ RNN ê¸°ë³¸ êµ¬ì¡°\n",
    "\n",
    "| êµ¬ë¶„ | ì„¤ëª… |\n",
    "|------|------|\n",
    "| **baseline** | ì¼ë°˜ ì‹ ê²½ë§ (Dense Layer) |\n",
    "| **Simple RNN** | ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient) ë¬¸ì œ ì¡´ì¬ |\n",
    "| **LSTM** | RNNì˜ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²° |\n",
    "| **Bidirectional LSTM** | ì–‘ë°©í–¥ìœ¼ë¡œ ë¬¸ë§¥ í•™ìŠµ |\n",
    "\n",
    "> ğŸ”¹ **RNN**ì€ ìˆœì„œ(ì‹œí€€ìŠ¤)ê°€ ìˆëŠ” ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ ì‚¬ìš©í•œë‹¤.  \n",
    "> ì˜ˆ: ë¬¸ì¥, ìŒì„±, ì‹œê³„ì—´ ë°ì´í„°\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“š í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê³¼ì •\n",
    "\n",
    "| ë‹¨ê³„ | ì„¤ëª… | ê´€ë ¨ í•¨ìˆ˜ |\n",
    "|------|------|-----------|\n",
    "| **1. Tokenization** | í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜ (ë‹¨ì–´ë³„ ì¸ë±ìŠ¤ ìƒì„±) | `Tokenizer()` |\n",
    "| **2. Word Embedding** | ë‹¨ì–´ë¥¼ ê³ ì°¨ì› ì‹¤ìˆ˜ ë²¡í„°ë¡œ ë³€í™˜ | `Embedding()` |\n",
    "| **3. Sequence Padding** | ë¬¸ì¥ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶¤ | `pad_sequences()` |\n",
    "\n",
    "---\n",
    "\n",
    "#### âš™ï¸ í† í¬ë‚˜ì´ì € 3ë‹¨ê³„ ìš”ì•½\n",
    "\n",
    "| ë‹¨ê³„ | í•¨ìˆ˜ | ê¸°ëŠ¥ |\n",
    "|------|------|------|\n",
    "| â‘  | `fit_on_texts(texts)` | ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ ìˆœì„œëŒ€ë¡œ **ì¸ë±ìŠ¤ ë”•ì…”ë„ˆë¦¬ êµ¬ì¶•** |\n",
    "| â‘¡ | `texts_to_sequences(texts)` | ê° ë¬¸ì„œë¥¼ **ì •ìˆ˜ ì‹œí€€ìŠ¤**ë¡œ ë³€í™˜ |\n",
    "| â‘¢ | `pad_sequences(sequences)` | ê¸¸ì´ë¥¼ ë§ì¶”ì–´ **íŒ¨ë”© ì²˜ë¦¬ (ë™ì¼í•œ ê¸¸ì´)** |\n",
    "\n",
    "> ğŸ’¡ `Tokenizer`ëŠ” ë‹¨ì–´ â†’ ìˆ«ì ì¸ì½”ë”©,  \n",
    "> `Embedding`ì€ ìˆ«ì â†’ ë²¡í„° ì¸ì½”ë”© ì—­í• ì„ í•¨.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§© ì„ë² ë”©(Embedding) Layer\n",
    "\n",
    "| í•­ëª© | ì„¤ëª… |\n",
    "|------|------|\n",
    "| **ì •ì˜** | ë‹¨ì–´ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ ì‹¤ìˆ˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì¸µ |\n",
    "| **ì…ë ¥ê°’** | ì •ìˆ˜ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ (í† í°í™” ê²°ê³¼ë¬¼) |\n",
    "| **ì¶œë ¥ê°’** | ê° ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ (ì˜ˆ: 128ì°¨ì›, 300ì°¨ì› ë“±) |\n",
    "| **ì—­í• ** | ëª¨ë¸ì´ ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•¨ |\n",
    "\n",
    "> ì˜ˆì‹œ:  \n",
    "> `\"dog\"` â†’ `[0.12, 0.45, -0.33, ...]`  \n",
    "> `\"cat\"` â†’ `[0.10, 0.43, -0.31, ...]`  \n",
    "> (ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ë‹¨ì–´ëŠ” ë²¡í„° ê³µê°„ìƒì—ì„œë„ ê°€ê¹ê²Œ ìœ„ì¹˜)\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§± ëª¨ë¸ ì¢…ë¥˜ë³„ ë¹„êµ\n",
    "\n",
    "| ëª¨ë¸ | íŠ¹ì§• | ì¥ë‹¨ì  |\n",
    "|------|------|--------|\n",
    "| **Simple RNN** | ê¸°ë³¸ RNN êµ¬ì¡° | ê³„ì‚° ë‹¨ìˆœí•˜ì§€ë§Œ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ì–´ë ¤ì›€ |\n",
    "| **LSTM** | ì…€ ìƒíƒœ(cell state)ë¡œ ì¥ê¸° ê¸°ì–µ ìœ ì§€ | ë³µì¡í•˜ì§€ë§Œ ì„±ëŠ¥ ìš°ìˆ˜ |\n",
    "| **Bidirectional LSTM** | ë¬¸ì¥ì„ ì•ë’¤ ì–‘ë°©í–¥ìœ¼ë¡œ í•™ìŠµ | ë¬¸ë§¥ íŒŒì•… ì •í™•ë„ ë†’ìŒ, ê³„ì‚°ëŸ‰ ë§ìŒ |\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ” RNN í•™ìŠµì˜ íë¦„\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "A[í…ìŠ¤íŠ¸ ë°ì´í„°] --> B[í† í°í™” (Tokenizer)]\n",
    "B --> C[ì •ìˆ˜ ì‹œí€€ìŠ¤ ë³€í™˜]\n",
    "C --> D[íŒ¨ë”© (pad_sequences)]\n",
    "D --> E[ì„ë² ë”© ë ˆì´ì–´]\n",
    "E --> F[Simple RNN / LSTM / Bi-LSTM]\n",
    "F --> G[Dense Layer ì¶œë ¥]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4596ada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/anaconda3/lib/python3.13/site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in /Users/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/anaconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /Users/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6c9a9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 2, 6, 3, 7, 4], [8, 2, 9, 10, 1], [3, 2, 1, 4]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "sample_reviews = [\n",
    "    \"this movie is great and wonderful\",\n",
    "    \"bad movie with poor acting\",\n",
    "    \"great movie absolutely wonderful\"\n",
    "]\n",
    "\n",
    "# 1. ë‹¨ì–´ ë¶„í•  ë° ë¹ˆë„ ê³„ì‚°\n",
    "all_reviews = []\n",
    "# [ all_reviews.extend(i) for i in [review.split() for review in sample_reviews] ]\n",
    "\n",
    "for i in [review.split() for review in sample_reviews]:\n",
    "    all_reviews.extend(i)\n",
    "    \n",
    "# 2. ë‹¨ì–´ë¹ˆë„\n",
    "word_freq = Counter(all_reviews)\n",
    "\n",
    "# 3. Tokenizer êµ¬í˜„\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self , num_reviews=10 , oov_token='UNK'):\n",
    "        self.num_reviews = num_reviews\n",
    "        self.oov_token = oov_token\n",
    "        self.review_index = {}\n",
    "        self.index_review = {}\n",
    "    def fit_on_texts(self , texts):\n",
    "        all_reviews = []\n",
    "        for i in [review.split() for review in sample_reviews]:\n",
    "            all_reviews.extend(i)\n",
    "        word_freq = Counter(all_reviews)\n",
    "        # ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìˆœì„œë¡œ index ë¶€ì—¬\n",
    "        # oov í† í°ì„ 1ë¡œ ì„¤ì •\n",
    "        self.review_index[self.oov_token] = 1\n",
    "        self.index_review[1] = self.oov_token\n",
    "        idx = 2\n",
    "        for word , _ in word_freq.most_common(self.num_reviews-1):\n",
    "            self.review_index[word] = idx\n",
    "            self.index_review[idx] = word\n",
    "            idx += 1\n",
    "    def texts_to_sequences(self , texts):\n",
    "        '''í…ìŠ¤íŠ¸ë¥¼ ì ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜'''\n",
    "        sequentces = []\n",
    "        for text in texts:\n",
    "            seq = []\n",
    "            for word in text.split():\n",
    "                # ë‹¨ì–´ê°€ vocabulary ì— ìˆìœ¼ë©´ indexë¥¼ ì‚¬ìš© / ì—†ìœ¼ë©´ oov\n",
    "                review_index = self.review_index.get(word,1)\n",
    "                seq.append(review_index)\n",
    "            sequentces.append(seq)\n",
    "        return sequentces\n",
    "    \n",
    "# Tokenizer  ìƒì„± ë° í•™ìŠµ\n",
    "tokenizer = SimpleTokenizer(num_reviews=10 , oov_token='UNK')\n",
    "tokenizer.fit_on_texts(sample_reviews)\n",
    "tokenizer.review_index\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "tokenizer.texts_to_sequences(sample_reviews) # 3ê°œì˜ ë¬¸ì¥ì„ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•¨ ( ë¬¸ìì˜ ê¸¸ì´ê°€ ë‹¤ ë‹¬ë¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18f3f39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 2, 6, 3, 7, 4], [8, 2, 9, 10, 1], [3, 2, 1, 4]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "sequences = tokenizer.texts_to_sequences(sample_reviews)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "986f9c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this movie is great and wonderful',\n",
       " 'bad movie with poor acting',\n",
       " 'great movie absolutely wonderful']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c50ba6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  5,  2,  6,  3,  7,  4],\n",
       "       [ 0,  0,  0,  0,  0,  8,  2,  9, 10,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  3,  2,  1,  4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# íŒ¨ë”© êµ¬í˜„ - ë¬¸ìì—´ì˜ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶˜ë‹¤\n",
    "def pad_sequence_manual(sequences, max_len=10, padding='pre',value=0):\n",
    "    '''íŒ¨ë”©êµ¬í˜„'''\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= max_len:\n",
    "            if padded == 'pre':\n",
    "                padded_seq = seq[-max_len:]\n",
    "            else:\n",
    "                padded_seq = seq[:max_len]\n",
    "        else:\n",
    "            pad_length = max_len-len(seq)\n",
    "            if padding == 'pre':\n",
    "                padded_seq = [value]*pad_length + seq\n",
    "            else:\n",
    "                padded_seq = seq + [value]*pad_length\n",
    "        padded.append(padded_seq)\n",
    "    return np.array(padded)\n",
    "padded = pad_sequence_manual(sequences)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5cde7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  5,  2,  6,  3,  7,  4],\n",
       "        [ 0,  0,  0,  0,  0,  8,  2,  9, 10,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  3,  2,  1,  4]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch tensor ë³€í™˜\n",
    "sequence_tensor =  torch.LongTensor(padded)\n",
    "sequence_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a45df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íŒ¨ë”©ëœ ì‹œì¿¼ìŠ¤ í˜•íƒœ : torch.Size([3, 10])\n",
      "ì²«ë²ˆì§¸ : tensor([0, 0, 0, 0, 5, 2, 6, 3, 7, 4])\n"
     ]
    }
   ],
   "source": [
    "# 2. ì›Œë“œ ì„ë² ë”© - ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch.nn as nn\n",
    "print(f'íŒ¨ë”©ëœ ì‹œì¿¼ìŠ¤ í˜•íƒœ : {sequence_tensor.shape}')\n",
    "print(f'ì²«ë²ˆì§¸ : {sequence_tensor[0]}')\n",
    "# pytorch embedding ë ˆì´ì–´ ìƒì„±\n",
    "# num_embeddings ì–´íœ˜ì˜ í¬ê¸°\n",
    "# embedding_dim ê° ë‹¨ì–´ë¥¼ ëª‡ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„í•  ê²ƒì¸ì§€\n",
    "# padding_idx ê¸¸ì´ë§ì¶œë•Œ ì±„ìš°ëŠ” ê°’\n",
    "embedding_layer = nn.Embedding(num_embeddings=1000,embedding_dim=8,padding_idx=0)\n",
    "embedded = embedding_layer(sequence_tensor)\n",
    "print(f'ì…ë ¥í˜•íƒœ : {sequence_tensor.shape}')\n",
    "print(f'ì¶œë ¥í˜•íƒœ : {embedded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29491d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² íŒ… ë²¡í„° ìƒì„¸ ë¶„ì„\n",
    "# ìƒ˜í”Œ ë°ì´í„°ì˜ ì²« 3ê°œ ë‹¨ì–´ ì„ë² ë”©\n",
    "for word_idx in range(3):\n",
    "    embedding_vec =  embedded[0,word_idx].detach().numpy()\n",
    "    word_id = sequence_tensor[0, word_idx].item()\n",
    "    print(f'ë‹¨ì–´ id { word_id} : {embedding_vec} -- 8ì°¨ì›ì¤‘ ì²˜ìŒ 4ê°œ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© í–‰ë ¬\n",
    "embedding_matrix =  embedding_layer.weight.detach().numpy()\n",
    "print(f'ì„ë² ë”© í–‰ë ¬ í˜•íƒœ : {embedding_matrix.shape}')  # 1000,8\n",
    "print(f'íŒ¨ë”©(id=0)ì˜ ì„ë² ë”© {embedding_matrix[0]}')\n",
    "print(f'ë‹¨ì–´ id=5ì¸ ì„ë² ë”© {embedding_matrix[5]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
