{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68542691",
   "metadata": {},
   "source": [
    "### ğŸ§  í…ìŠ¤íŠ¸ ë¶„ì„ í•µì‹¬ ì •ë¦¬\n",
    "\n",
    "#### ğŸ“¦ 1. BOW (Bag of Words)\n",
    "- ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜ ê¸°ë°˜ í‘œí˜„ (`CountVectorizer`)\n",
    "- **ì¥ì :** ë¹ ë¥´ê³  ê°„ë‹¨  \n",
    "- **ë‹¨ì :** ë‹¨ì–´ ìˆœì„œ, ì˜ë¯¸ ì†ì‹¤ / í¬ì†Œí–‰ë ¬ ë°œìƒ\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“˜ 2. TF-IDF ì •ë¦¬\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§© 1ï¸âƒ£ TF (Term Frequency, ë‹¨ì–´ ë¹ˆë„)\n",
    "**ëœ»:**  \n",
    "í•œ ë¬¸ì„œ ì•ˆì—ì„œ íŠ¹ì • ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ìì£¼ ë“±ì¥í–ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„.  \n",
    "ì¦‰, ë‹¨ì–´ê°€ ë§ì´ ë“±ì¥í• ìˆ˜ë¡ ê·¸ ë¬¸ì„œì—ì„œ **ì¤‘ìš”í•œ ë‹¨ì–´**ë¼ê³  ë³´ëŠ” ê°œë….\n",
    "\n",
    "> âœ… ì˜ˆì‹œ:  \n",
    "> \"ê³ ì–‘ì´ê°€ ê·€ì—½ë‹¤. ê³ ì–‘ì´ëŠ” ì‚¬ë‘ìŠ¤ëŸ½ë‹¤.\"  \n",
    "> â†’ \"ê³ ì–‘ì´\"ì˜ TF ê°’ì´ ë†’ìŒ (ë¬¸ì„œ ë‚´ì—ì„œ ìì£¼ ë“±ì¥í•˜ê¸° ë•Œë¬¸)\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§® 2ï¸âƒ£ IDF (Inverse Document Frequency, ì—­ë¬¸ì„œ ë¹ˆë„)\n",
    "**ëœ»:**  \n",
    "ëª¨ë“  ë¬¸ì„œì—ì„œ ë„ˆë¬´ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” **ì¤‘ìš”í•˜ì§€ ì•Šë‹¤**ê³  ë³´ëŠ” ê°œë….  \n",
    "ì¦‰, ëª¨ë“  ë¬¸ì„œì— ê³µí†µì ìœ¼ë¡œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì˜ **ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì´ëŠ” ì—­í• **ì„ í•¨.\n",
    "\n",
    "> âœ… ì˜ˆì‹œ:  \n",
    "> \"ê·¸ë¦¬ê³ \", \"ì´ë‹¤\", \"í•˜ì§€ë§Œ\" ê°™ì€ ë‹¨ì–´ëŠ”  \n",
    "> ê±°ì˜ ëª¨ë“  ë¬¸ì„œì— ë“±ì¥í•˜ë¯€ë¡œ IDF ê°’ì´ ë‚®ìŒ â†’ ì¤‘ìš”ë„ â†“\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§  3ï¸âƒ£ TF-IDF (Term Frequency Ã— Inverse Document Frequency)\n",
    "**ëœ»:**  \n",
    "TFì™€ IDFë¥¼ ê³±í•´ì„œ  \n",
    "â€œ**íŠ¹ì • ë¬¸ì„œì—ì„œëŠ” ìì£¼ ë“±ì¥í•˜ì§€ë§Œ, ì „ì²´ ë¬¸ì„œì—ì„œëŠ” ë“œë¬¸ ë‹¨ì–´**â€ì— ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ì‹.\n",
    "\n",
    "> âœ… í•µì‹¬ í¬ì¸íŠ¸:  \n",
    "> - TF â†’ ë¬¸ì„œ ë‚´ë¶€ì—ì„œì˜ ì¤‘ìš”ë„  \n",
    "> - IDF â†’ ì „ì²´ ë¬¸ì„œ ì¤‘ í¬ê·€ì„±  \n",
    "> - TF-IDF = **ì¤‘ìš”í•˜ë©´ì„œë„ í¬ê·€í•œ ë‹¨ì–´** ê°•ì¡°\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ“ **ìš”ì•½ ê³µì‹**\n",
    "\n",
    "\\[\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\log\\left(\\frac{N}{DF(t)}\\right)\n",
    "\\]\n",
    "\n",
    "- \\( t \\): ë‹¨ì–´ (term)  \n",
    "- \\( d \\): ë¬¸ì„œ (document)  \n",
    "- \\( N \\): ì „ì²´ ë¬¸ì„œ ìˆ˜  \n",
    "- \\( DF(t) \\): ë‹¨ì–´ \\( t \\)ê°€ ë“±ì¥í•œ ë¬¸ì„œ ìˆ˜  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸ§¾ **ê²°ë¡ :**  \n",
    "TF-IDFëŠ” ë¬¸ì„œ ë‚´ ë‹¨ì–´ì˜ **ë¹ˆë„(TF)** ì™€  \n",
    "ì „ì²´ ë¬¸ì„œ ë‚´ **í¬ê·€ì„±(IDF)** ì„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬  \n",
    "**ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œ**ë¥¼ ì°¾ëŠ” ëŒ€í‘œì ì¸ ë°©ë²•ì´ë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“Š 3. Multinomial Naive Bayes\n",
    "- ë‹¨ì–´ í™•ë¥  ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸  \n",
    "- **íŠ¹ì§•:** ë¹ ë¥´ê³  ë‹¨ìˆœ / BOWÂ·TF-IDFì™€ ê¶í•© ì¢‹ìŒ\n",
    "\n",
    "---\n",
    "\n",
    "#### âš™ï¸ 4. Logistic Regression\n",
    "- í™•ë¥  ê¸°ë°˜ íšŒê·€í˜• ë¶„ë¥˜ ëª¨ë¸  \n",
    "- **ë‹¤ì¤‘ í´ë˜ìŠ¤ ì§€ì›:** `multi_class='multinomial'`  \n",
    "- **ì¥ì :** ì•ˆì •ì , í•´ì„ ìš©ì´\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§© 5. Ridge Classifier\n",
    "- L2 ê·œì œ ì ìš©ëœ íšŒê·€ ê¸°ë°˜ ë¶„ë¥˜  \n",
    "- **ì—­í• :** ê³¼ì í•© ë°©ì§€, ë‹¨ìˆœí•œ ì„ í˜• ëª¨ë¸\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”  6. N-gram\n",
    "- ì—°ì†ëœ Nê°œì˜ ë‹¨ì–´ ë¬¶ìŒ (ì˜ˆ: \"ë„ˆë¬´ ë§›ìˆë‹¤\" â†’ 2-gram)  \n",
    "- **ì¥ì :** ë¬¸ë§¥ ë°˜ì˜  \n",
    "- **ë‹¨ì :** ì°¨ì› í­ë°œ â†’ ì°¨ì› ì¶•ì†Œ í•„ìš”\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ‡°ğŸ‡· 7. KoNLPy (Okt)\n",
    "- í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°  \n",
    "- ë¬¸ì¥ì„ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (â€œë°¥ì„ ë¨¹ì—ˆë‹¤â€ â†’ [ë°¥, ë¨¹, ë‹¤])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4407c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize      # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ ë¶„ë¦¬\n",
    "from nltk.stem import PorterStemmer          # ì–´ê°„(ê¸°ë³¸í˜•) ì¶”ì¶œê¸°\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# fetch_20newsgroups : 20ê°œì˜ ë‰´ìŠ¤ í† í”½(ì¹´í…Œê³ ë¦¬)ì— ê±¸ì¹œ ì•½ 18,000ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ ëŒ€í‘œì ì¸ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë°ì´í„°ì…‹\n",
    "# (ì˜ˆ: ì»´í“¨í„°, ìŠ¤í¬ì¸ , ì •ì¹˜, ì¢…êµ ë“±)\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression , RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e0775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "newsgroup_train = fetch_20newsgroups(subset='train',\n",
    "                remove=('headers','footers','quotes'),\n",
    "                categories=categories\n",
    "                )\n",
    "newsgroup_test = fetch_20newsgroups(subset='test',\n",
    "                remove=('headers','footers','quotes'),\n",
    "                categories=categories\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d1a071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í›ˆë ¨ ë°ì´í„° í¬ê¸°: 2034\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: 1353\n",
      "\n",
      "ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ(ì²˜ìŒ 200ì):\n",
      " Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and o\n"
     ]
    }
   ],
   "source": [
    "# í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
    "train_data = newsgroup_train.data  # ì‹¤ì œ í…ìŠ¤íŠ¸ ë°ì´í„°\n",
    "test_data = newsgroup_test.data\n",
    "\n",
    "print(\"í›ˆë ¨ ë°ì´í„° í¬ê¸°:\", len(train_data))\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°:\", len(test_data))\n",
    "print(\"\\nì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ(ì²˜ìŒ 200ì):\\n\", train_data[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67955925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë°ì´í„° í™•ì¸ ===\n",
      "ë°ì´í„° íƒ€ì…: <class 'list'>\n",
      "ë°ì´í„° ê¸¸ì´: 2034\n",
      "\n",
      "ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ:\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and o\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° í™•ì¸\n",
    "print(\"=== ë°ì´í„° í™•ì¸ ===\")\n",
    "print(f\"ë°ì´í„° íƒ€ì…: {type(train_data)}\")\n",
    "print(f\"ë°ì´í„° ê¸¸ì´: {len(train_data)}\")\n",
    "print(f\"\\nì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ:\\n{train_data[0][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707de127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•´ë” í‘¸í„° ì¸ìš©ë¬¸ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a2024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # í—¤ë” ì œê±°\n",
    "    text = re.sub(r'^From:.*\\n', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^Subject:.*\\n', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # í’‹í„° ì œê±°\n",
    "    text = re.sub(r'\\n--\\n.*$', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # ì¸ìš©ë¬¸ ì œê±°\n",
    "    text = re.sub(r'(^|\\n)[>|:].*', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b4dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [ clean_text(t) for t in train_data]\n",
    "test_data = [ clean_text(t) for t in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f44098d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 1353)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) , len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e21d94",
   "metadata": {},
   "source": [
    "## ğŸ§® ë©€í‹°ë…¸ë¯¸ì–¼ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ (Multinomial Naive Bayes)\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“˜ ê°œë…\n",
    "**ë©€í‹°ë…¸ë¯¸ì–¼ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ(Multinomial Naive Bayes)** ëŠ”  \n",
    "ë¬¸ì„œì— í¬í•¨ëœ **ë‹¨ì–´ë“¤ì˜ ì¶œí˜„ íšŸìˆ˜**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ  \n",
    "ê·¸ ë¬¸ì„œê°€ **ì–´ë–¤ ì£¼ì œ(ì¹´í…Œê³ ë¦¬)** ì— ì†í• ì§€ë¥¼  \n",
    "**í™•ë¥ ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜**ì´ë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§  í•µì‹¬ ì›ë¦¬ â€” ë² ì´ì¦ˆ ì •ë¦¬ (ì¡°ê±´ë¶€ í™•ë¥ )\n",
    "ì´ ëª¨ë¸ì€ **ë² ì´ì¦ˆ ì •ë¦¬(Bayesâ€™ Theorem)** ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë™í•œë‹¤.\n",
    "\n",
    "> ğŸ“ íŠ¹ì • ë‹¨ì–´ Aê°€ ì£¼ì–´ì¡Œì„ ë•Œ,  \n",
    "> ê·¸ ë¬¸ì„œê°€ ìŠ¤íŒ¸(B)ì¼ í™•ë¥ ì„ ê³„ì‚°í•œë‹¤ëŠ” ê°œë…  \n",
    "> â†’ **P(B | A)** = ë‹¨ì–´ Aê°€ ë“±ì¥í–ˆì„ ë•Œ, ë¬¸ì„œê°€ ìŠ¤íŒ¸ Bì¼ í™•ë¥ \n",
    "\n",
    "\\[\n",
    "P(B|A) = \\frac{P(A|B) \\times P(B)}{P(A)}\n",
    "\\]\n",
    "\n",
    "ì¦‰,  \n",
    "â€œë¬¸ì„œ ì•ˆì— ìˆëŠ” ë‹¨ì–´ë“¤ì´ íŠ¹ì • ì£¼ì œì—ì„œ ì–¼ë§ˆë‚˜ ìì£¼ ë“±ì¥í•˜ëŠ”ê°€â€ë¥¼  \n",
    "í™•ë¥ ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ë¬¸ì„œì˜ ì£¼ì œë¥¼ ì˜ˆì¸¡í•œë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "#### âš™ï¸ ì‘ë™ ê³¼ì •\n",
    "1. **í›ˆë ¨ ë°ì´í„°ì—ì„œ ê° ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ ê³„ì‚°**  \n",
    "   â†’ ë¬¸ì„œë³„ ë‹¨ì–´ ì¶œí˜„ íšŸìˆ˜(Count Vector)ë¥¼ ì‚¬ìš©  \n",
    "2. **ê° ë‹¨ì–´ê°€ íŠ¹ì • í´ë˜ìŠ¤(ì˜ˆ: ìŠ¤íŒ¸)ì— ë“±ì¥í•  í™•ë¥  ê³„ì‚°**  \n",
    "   â†’ ì¡°ê±´ë¶€ í™•ë¥  \\( P(\\text{Word} | \\text{Class}) \\)  \n",
    "3. **ìƒˆë¡œìš´ ë¬¸ì„œ ì…ë ¥ ì‹œ, ë‹¨ì–´ë“¤ì˜ í™•ë¥ ì„ ëª¨ë‘ ê³±í•˜ì—¬**  \n",
    "   â†’ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜  \n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“Š í™œìš© ë¶„ì•¼\n",
    "| ë¶„ì•¼ | ì„¤ëª… |\n",
    "|------|------|\n",
    "| ğŸ“§ ìŠ¤íŒ¸ í•„í„°ë§ | ë‹¨ì–´ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤íŒ¸ ì—¬ë¶€ íŒë‹¨ |\n",
    "| ğŸ“° ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ë¥˜ | ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ ë“± ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ |\n",
    "| ğŸ’¬ ê°ì„± ë¶„ì„ | ë¦¬ë·°ë‚˜ ëŒ“ê¸€ì˜ ê¸ì •/ë¶€ì • ê°ì • íŒë³„ |\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§© ì˜ˆì‹œ ë¬¸ì¥ìœ¼ë¡œ ì´í•´í•˜ê¸°\n",
    "> â€œê´‘ê³  í´ë¦­í•˜ì„¸ìš”â€ë¼ëŠ” ë¬¸ì¥ì´ ìˆì„ ë•Œ  \n",
    "> - â€œê´‘ê³ â€, â€œí´ë¦­â€ ë“±ì˜ ë‹¨ì–´ê°€ **ìŠ¤íŒ¸ ë¬¸ì„œì—ì„œ ìì£¼ ë“±ì¥**í–ˆë‹¤ë©´  \n",
    ">   â†’ P(ìŠ¤íŒ¸ | ê´‘ê³ , í´ë¦­) ê°’ì´ ë†’ì•„ì ¸ **ìŠ¤íŒ¸ìœ¼ë¡œ ë¶„ë¥˜**\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ§¾ ê²°ë¡ \n",
    "> ë©€í‹°ë…¸ë¯¸ì–¼ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆëŠ”  \n",
    "> **ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜ + ì¡°ê±´ë¶€ í™•ë¥ (ë² ì´ì¦ˆ ì •ë¦¬)** ë¥¼ ì´ìš©í•´  \n",
    "> ë¬¸ì„œê°€ ì†í•  **ì£¼ì œë‚˜ ê°ì • ìƒíƒœë¥¼ í™•ë¥ ì ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ëª¨ë¸**ì´ë‹¤.  \n",
    "> ë‹¨ìˆœí•˜ì§€ë§Œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë¶„ì•¼ì—ì„œ **ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ê¸°ë³¸ ëª¨ë¸**ë¡œ ìì£¼ ì‚¬ìš©ëœë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d625d7",
   "metadata": {},
   "source": [
    "#### ë‚˜ì´ë¸Œ naive : ìˆœì§„í•œ ê°€ì •\n",
    "#### ê°€ì • : ë¬¸ì„œì•ˆì˜ ëª¨ë“  ë‹¨ì–´ëŠ” ì„œë¡œ ë…ë¦½ì \n",
    "#### í˜„ì‹¤ : ìŠ¤íŒ¸ì— ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì€ ì„œë¡œ ë…ë¦½ì ì´ì§€ ì•Šë‹¤.\n",
    "#### ì‹¤ì œ : ì´ëŸ¬í•œ ê°€ì •ì€ ê³„ì‚°ëŸ‰ì„ ë¹ ë¥´ê²Œ í•˜ê³  ë‹¨ìˆœí•˜ì§€ë§Œ ì •í™•ë„ê°€ ì–´ëŠì •ë„ ë‚˜ì˜¨ë‹¤.\n",
    "#### ë©€í‹°ë…¸ë©€ : ë‹¤í•­ ë¶„í¬\n",
    "#### ì˜ë¯¸ : ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜ë¥¼ ì¤‘ìš”í•˜ê²Œ ë´„.\n",
    "#### íšŸìˆ˜ë¥¼ ì„¸ëŠ” ë©€í‹°ë…¸ë¯¸ì–¼ ë°©ì‹ì´ NLP ë°©ì‹ì´ ì˜ ë§ëŠ”ë‹¤.\n",
    "#### ìŠ¤íŒ¸ë©”ì¼í†µê³„ (SPEM) - free , money , viagra , report ë“±ë“± ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ê°€ì§€ê³  í†µê³„ë¥¼ ëƒ„.\n",
    "#### ì •ìƒë©”ì¼í†µê³„ (HAM) - free , money , viagra , report ë“±ë“± ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ê°€ì§€ê³  í†µê³„ë¥¼ ëƒ„.\n",
    "#### ì´ëŸ¬í•œ í†µê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ ì¹´í…Œê³ ë¦¬ì—ì„œ íŠ¹ì • ë‹¨ì˜¤ê°€ ë‚˜ì˜¬ í™•ë¥  P('free'|ìŠ¤íŒ¸ë©”ì¼) ì„ ê³„ì‚°í•˜ëŠ” ê²ƒ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f65785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kangminji/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0aad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faad9fa",
   "metadata": {},
   "source": [
    "##### max_features=2000 \"ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´ 2,000ê°œê¹Œì§€ë§Œ ì‚¬ìš©í•˜ê² ë‹¤\" ëŠ” ëœ»\n",
    "##### min_df \"ë„ˆë¬´ ë“œë¬¼ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ë¬´ì‹œí•˜ê² ë‹¤\" ëŠ” ëœ»\n",
    "##### max_df \"ë„ˆë¬´ í”í•˜ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ë¬´ì‹œí•˜ê² ë‹¤\" ëŠ” ëœ»\n",
    "\n",
    "| íŒŒë¼ë¯¸í„°           | ëœ»               | ì—­í•                    |\n",
    "| -------------- | --------------- | -------------------- |\n",
    "| `max_features` | ì‚¬ìš©í•  **ìµœëŒ€ ë‹¨ì–´ ìˆ˜** | ë„ˆë¬´ ë§ì€ ë‹¨ì–´ â†’ ìƒìœ„ ë‹¨ì–´ë§Œ ì‚¬ìš© |\n",
    "| `min_df`       | **ìµœì†Œ ë“±ì¥ ë¬¸ì„œ ìˆ˜**  | ë„ˆë¬´ ë“œë¬¸ ë‹¨ì–´ â†’ ì œê±°        |\n",
    "| `max_df`       | **ìµœëŒ€ ë“±ì¥ ë¬¸ì„œ ë¹„ìœ¨** | ë„ˆë¬´ í”í•œ ë‹¨ì–´ â†’ ì œê±°        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bda091ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë²¡í„°í™” ê²°ê³¼ ===\n",
      "í›ˆë ¨ ë°ì´í„° í˜•íƒœ: (2034, 2000)\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° í˜•íƒœ: (1353, 2000)\n",
      "\n",
      "=== ì¶”ì¶œëœ ë‹¨ì–´ ìƒ˜í”Œ ===\n",
      "ìƒìœ„ 10ê°œ ë‹¨ì–´: ['hi', 've', 'save', 'model', 'mapping', 'planes', 'file', 'given', 'default', 'position']\n",
      "\n",
      "=== íŠ¹ì„± ì •ë³´ ===\n",
      "ì´ ë‹¨ì–´ ìˆ˜: 2000\n"
     ]
    }
   ],
   "source": [
    "# í…ìŠ¤íŠ¸ ë²¡í„°í™”ë¥¼ ìœ„í•œ CountVectorizer ì„¤ì •\n",
    "cv = CountVectorizer(\n",
    "    max_features=2000,    # ê°€ì¥ ë¹ˆë„ê°€ ë†’ì€ 2000ê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©\n",
    "    min_df=2,            # ìµœì†Œ 2ê°œ ë¬¸ì„œì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë§Œ í¬í•¨\n",
    "    max_df=0.95,         # ì „ì²´ ë¬¸ì„œì˜ 95% ì´í•˜ì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë§Œ í¬í•¨\n",
    "    stop_words='english'  # ì˜ì–´ ë¶ˆìš©ì–´ ì œê±°\n",
    ")\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„° ë²¡í„°í™”\n",
    "x_train_cv = cv.fit_transform(train_data)\n",
    "x_test_cv = cv.transform(test_data)\n",
    "\n",
    "# ë²¡í„°í™” ê²°ê³¼ ì¶œë ¥\n",
    "print(\"=== ë²¡í„°í™” ê²°ê³¼ ===\")\n",
    "print(\"í›ˆë ¨ ë°ì´í„° í˜•íƒœ:\", x_train_cv.shape)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í˜•íƒœ:\", x_test_cv.shape)\n",
    "print(\"\\n=== ì¶”ì¶œëœ ë‹¨ì–´ ìƒ˜í”Œ ===\")\n",
    "print(\"ìƒìœ„ 10ê°œ ë‹¨ì–´:\", list(cv.vocabulary_.keys())[:10])\n",
    "print(\"\\n=== íŠ¹ì„± ì •ë³´ ===\")\n",
    "print(\"ì´ ë‹¨ì–´ ìˆ˜:\", len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63ac86a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===\n",
      "í›ˆë ¨ ë°ì´í„° ì •í™•ë„: 0.8338\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •í™•ë„: 0.7369\n",
      "\n",
      "=== ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.61      0.62      0.61       319\n",
      "talk.religion.misc       0.88      0.87      0.88       389\n",
      "     comp.graphics       0.80      0.82      0.81       394\n",
      "         sci.space       0.58      0.55      0.57       251\n",
      "\n",
      "          accuracy                           0.74      1353\n",
      "         macro avg       0.72      0.71      0.72      1353\n",
      "      weighted avg       0.74      0.74      0.74      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train_cv, newsgroup_train.target)\n",
    "\n",
    "# ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
    "train_score = nb.score(x_train_cv, newsgroup_train.target)  # í›ˆë ¨ ë°ì´í„° ì„±ëŠ¥\n",
    "test_score = nb.score(x_test_cv, newsgroup_test.target)    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥\n",
    "\n",
    "print(\"=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° ì •í™•ë„: {train_score:.4f}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •í™•ë„: {test_score:.4f}\")\n",
    "\n",
    "# ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_nb = nb.predict(x_test_cv)\n",
    "print(\"\\n=== ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===\")\n",
    "print(classification_report(newsgroup_test.target, y_pred_nb, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe1a3613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8544739429695182 0.7354028085735402\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + MNB + LogisticRegression\n",
    "# TF-IDFë¡œ ì¤‘ìš”ë‹¨ì–´ ê°•ì¡°, ì„ í˜•ëª¨ë¸ê³¼ ìì£¼ì‚¬ìš©  BOW ëŒ€ë¹„ í”í•œ ë‹¨ì–´ ì˜í–¥ ê°ì†Œ\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "x_train_tfid = tfidf.fit_transform(train_data)\n",
    "x_test_tfid = tfidf.transform(test_data)\n",
    "\n",
    "# NB + tf-idf \n",
    "nb_tfidf  = MultinomialNB()\n",
    "nb_tfidf.fit(x_train_tfid,newsgroup_train.target)\n",
    "print(nb_tfidf.score(x_train_tfid,newsgroup_train.target),  nb_tfidf.score(x_test_tfid, newsgroup_test.target) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6613e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression ì„±ëŠ¥ í‰ê°€ ===\n",
      "í›ˆë ¨ ë°ì´í„° ì •í™•ë„: 0.9213\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •í™•ë„: 0.7354\n",
      "\n",
      "=== ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.63      0.62      0.62       319\n",
      "talk.religion.misc       0.79      0.90      0.84       389\n",
      "     comp.graphics       0.79      0.81      0.80       394\n",
      "         sci.space       0.67      0.50      0.57       251\n",
      "\n",
      "          accuracy                           0.74      1353\n",
      "         macro avg       0.72      0.71      0.71      1353\n",
      "      weighted avg       0.73      0.74      0.73      1353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with regularization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜ (L2 ê·œì œ ì ìš©)\n",
    "lr = LogisticRegression(\n",
    "    C=1.0,               # ê·œì œ ê°•ë„ì˜ ì—­ìˆ˜ (ì‘ì„ìˆ˜ë¡ ê·œì œê°€ ê°•í•¨)\n",
    "    max_iter=1000,       # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜\n",
    "    multi_class='multinomial',  # ë‹¤ì¤‘ í´ë˜ìŠ¤ ì„¤ì •\n",
    "    solver='lbfgs',      # ìµœì í™” ì•Œê³ ë¦¬ì¦˜\n",
    "    random_state=42      # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "lr.fit(x_train_tfid, newsgroup_train.target)\n",
    "\n",
    "# ì„±ëŠ¥ í‰ê°€\n",
    "train_score = lr.score(x_train_tfid, newsgroup_train.target)\n",
    "test_score = lr.score(x_test_tfid, newsgroup_test.target)\n",
    "\n",
    "print(\"=== Logistic Regression ì„±ëŠ¥ í‰ê°€ ===\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° ì •í™•ë„: {train_score:.4f}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •í™•ë„: {test_score:.4f}\")\n",
    "\n",
    "# ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
    "y_pred = lr.predict(x_test_tfid)\n",
    "print(\"\\n=== ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===\")\n",
    "print(classification_report(newsgroup_test.target, y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0393b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ridge Classifier ì„±ëŠ¥ í‰ê°€ ===\n",
      "í›ˆë ¨ ë°ì´í„° ì •í™•ë„: 0.9641\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •í™•ë„: 0.7147\n",
      "\n",
      "=== ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.62      0.59      0.61       319\n",
      "talk.religion.misc       0.83      0.84      0.84       389\n",
      "     comp.graphics       0.72      0.80      0.76       394\n",
      "         sci.space       0.61      0.53      0.57       251\n",
      "\n",
      "          accuracy                           0.71      1353\n",
      "         macro avg       0.70      0.69      0.69      1353\n",
      "      weighted avg       0.71      0.71      0.71      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ridge Classifierë¡œ ê³¼ì í•© í•´ê²°\n",
    "rc = RidgeClassifier(\n",
    "    alpha=0.5,          # ê·œì œ ê°•ë„ (ë†’ì„ìˆ˜ë¡ ê·œì œê°€ ê°•í•¨)\n",
    "    random_state=42     # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "rc.fit(x_train_tfid, newsgroup_train.target)\n",
    "\n",
    "# ì„±ëŠ¥ í‰ê°€\n",
    "train_score = rc.score(x_train_tfid, newsgroup_train.target)\n",
    "test_score = rc.score(x_test_tfid, newsgroup_test.target)\n",
    "\n",
    "print(\"=== Ridge Classifier ì„±ëŠ¥ í‰ê°€ ===\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° ì •í™•ë„: {train_score:.4f}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •í™•ë„: {test_score:.4f}\")\n",
    "\n",
    "# ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
    "y_pred = rc.predict(x_test_tfid)\n",
    "print(\"\\n=== ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===\")\n",
    "print(classification_report(newsgroup_test.target, y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "023b83a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 LogisticRegression ì ìˆ˜ (í›ˆë ¨, í…ŒìŠ¤íŠ¸): 0.8024, 0.7014\n",
      "\n",
      "=== íŠ¹ì„± ì„ íƒ íš¨ê³¼ ===\n",
      "ì „ì²´ íŠ¹ì„± ìˆ˜: 2000\n",
      "ì„ íƒëœ íŠ¹ì„± ìˆ˜: 302\n",
      "ì œê±°ëœ íŠ¹ì„± ìˆ˜: 1698\n",
      "íŠ¹ì„± ê°ì†Œìœ¨: 84.9%\n"
     ]
    }
   ],
   "source": [
    "# L1 ê·œì œ L1 LogisticRegression (Lassoì™€ ìœ ì‚¬)\n",
    "# ì¼ë¶€ ê³„ìˆ˜ë¥¼ 0 ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ íŠ¹ì„± ì„ íƒì„ ìˆ˜í–‰. \n",
    "# ì¤‘ìš”í”¼ì²˜ select íš¨ê³¼\n",
    "\n",
    "l1_lr = LogisticRegression(penalty='l1', max_iter=1000, solver='saga', random_state=42)\n",
    "l1_lr.fit(x_train_tfid, newsgroup_train.target)\n",
    "print(f\"L1 LogisticRegression ì ìˆ˜ (í›ˆë ¨, í…ŒìŠ¤íŠ¸): {l1_lr.score(x_train_tfid, newsgroup_train.target):.4f}, {l1_lr.score(x_test_tfid, newsgroup_test.target):.4f}\")\n",
    "\n",
    "# íŠ¹ì„± ì„ íƒ íš¨ê³¼ í™•ì¸\n",
    "n_features = x_train_tfid.shape[1]\n",
    "n_nonzero = np.sum(l1_lr.coef_ != 0)\n",
    "print(f\"\\n=== íŠ¹ì„± ì„ íƒ íš¨ê³¼ ===\")\n",
    "print(f\"ì „ì²´ íŠ¹ì„± ìˆ˜: {n_features}\")\n",
    "print(f\"ì„ íƒëœ íŠ¹ì„± ìˆ˜: {n_nonzero}\")\n",
    "print(f\"ì œê±°ëœ íŠ¹ì„± ìˆ˜: {n_features - n_nonzero}\")\n",
    "print(f\"íŠ¹ì„± ê°ì†Œìœ¨: {(n_features - n_nonzero) / n_features * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de63446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree ì ìˆ˜ (í›ˆë ¨, í…ŒìŠ¤íŠ¸): 0.4341, 0.4035\n",
      "\n",
      "=== Decision Tree ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.00      0.00      0.00       319\n",
      "talk.religion.misc       0.34      0.96      0.51       389\n",
      "     comp.graphics       0.83      0.29      0.43       394\n",
      "         sci.space       0.45      0.23      0.31       251\n",
      "\n",
      "          accuracy                           0.40      1353\n",
      "         macro avg       0.41      0.37      0.31      1353\n",
      "      weighted avg       0.43      0.40      0.33      1353\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Decision Tree - ìƒìœ„ 10ê°œ ì¤‘ìš” ë‹¨ì–´:\n",
      "parallel, essentially, defined, col, revelation, must, separate, constant, series, 24\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Random Forest ì ìˆ˜ (í›ˆë ¨, í…ŒìŠ¤íŠ¸): 0.6254, 0.5706\n",
      "\n",
      "=== Random Forest ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.63      0.30      0.41       319\n",
      "talk.religion.misc       0.54      0.92      0.68       389\n",
      "     comp.graphics       0.60      0.73      0.66       394\n",
      "         sci.space       0.59      0.12      0.20       251\n",
      "\n",
      "          accuracy                           0.57      1353\n",
      "         macro avg       0.59      0.52      0.49      1353\n",
      "      weighted avg       0.59      0.57      0.52      1353\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Random Forest - ìƒìœ„ 10ê°œ ì¤‘ìš” ë‹¨ì–´:\n",
      "series, mission, arguments, safety, lewis, constant, michael, science, cost, 24\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting ì ìˆ˜ (í›ˆë ¨, í…ŒìŠ¤íŠ¸): 0.9263, 0.6866\n",
      "\n",
      "=== Gradient Boosting ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.54      0.61      0.57       319\n",
      "talk.religion.misc       0.85      0.79      0.82       389\n",
      "     comp.graphics       0.72      0.79      0.75       394\n",
      "         sci.space       0.59      0.46      0.52       251\n",
      "\n",
      "          accuracy                           0.69      1353\n",
      "         macro avg       0.67      0.66      0.67      1353\n",
      "      weighted avg       0.69      0.69      0.69      1353\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Gradient Boosting - ìƒìœ„ 10ê°œ ì¤‘ìš” ë‹¨ì–´:\n",
      "michael, idea, lewis, cost, sense, science, wish, gone, series, 24\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# íŠ¸ë¦¬ëª¨ë¸ + tfidf\n",
    "# DecisionTree, RandomForest, GradientBoosting ëª¨ë¸ ì •ì˜\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "forest = RandomForestClassifier(max_depth=3, n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(max_depth=3, n_estimators=100, random_state=42)\n",
    "\n",
    "# ê° ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ í‰ê°€\n",
    "models = [\n",
    "    (\"Decision Tree\", tree),\n",
    "    (\"Random Forest\", forest),\n",
    "    (\"Gradient Boosting\", gb)\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    model.fit(x_train_tfid, newsgroup_train.target)\n",
    "    \n",
    "    # í•œ ì¤„ë¡œ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì ìˆ˜ í™•ì¸\n",
    "    train_score = model.score(x_train_tfid, newsgroup_train.target)\n",
    "    test_score = model.score(x_test_tfid, newsgroup_test.target)\n",
    "    print(f\"{name} ì ìˆ˜ (í›ˆë ¨, í…ŒìŠ¤íŠ¸): {train_score:.4f}, {test_score:.4f}\")\n",
    "    \n",
    "    # ì˜ˆì¸¡ ë° ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
    "    y_pred = model.predict(x_test_tfid)\n",
    "    print(f\"\\n=== {name} ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ===\")\n",
    "    print(classification_report(newsgroup_test.target, y_pred, target_names=categories))\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "    # íŠ¹ì„± ì¤‘ìš”ë„ ìƒìœ„ 10ê°œ ì¶œë ¥ (í…ìŠ¤íŠ¸ ë¶„ë¥˜ì—ì„œ ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ í™•ì¸)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = model.feature_importances_\n",
    "        feature_names = np.array(list(tfidf.vocabulary_.keys()))\n",
    "        top_features = feature_names[np.argsort(feature_importance)[-10:]]\n",
    "        print(f\"{name} - ìƒìœ„ 10ê°œ ì¤‘ìš” ë‹¨ì–´:\")\n",
    "        print(\", \".join(top_features))\n",
    "        print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67a359d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬\n",
    "# RegexpTokenizer + stopwords + PorterSemmer\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "regtok = RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    toks = regtok.tokenize(text.lower())\n",
    "    toks = [ t for t in toks if t not in english_stops ]\n",
    "    tokes = [ PorterStemmer().stem for t in toks ]\n",
    "    return tokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb36f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°•ì‚¬ë‹˜ ê¹ƒí—ˆë¸Œì™€ ë¹„êµí•˜ì—¬ ì½”ë“œ ì‘ì„± ë§ˆë¬´ë¦¬í•˜ê¸°."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
