{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "015c11c2",
   "metadata": {},
   "source": [
    "| ë¼ì´ë¸ŒëŸ¬ë¦¬                                                       | ì£¼ìš” íŠ¹ì§•                                    | ì¥ì                                                                                                         | ë‹¨ì                                                                                          | ì£¼ìš” ìš©ë„                                                                           |\n",
    "| :---------------------------------------------------------- | :--------------------------------------- | :-------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ |\n",
    "| **TextBlob**                                                | ì‚¬ì „ ê¸°ë°˜ ê°ì„± ë¶„ì„ + í’ˆì‚¬ íƒœê¹…, ëª…ì‚¬êµ¬ ì¶”ì¶œ ë“± NLP ê¸°ëŠ¥ ì œê³µ  | - ì‚¬ìš©ì´ ë§¤ìš° ê°„ë‹¨í•¨ (`TextBlob(text).sentiment`)  <br> - ë¬¸ë²• êµì •, ë²ˆì—­, í’ˆì‚¬ ë¶„ì„ ë“± ì¶”ê°€ ê¸°ëŠ¥ í’ë¶€ <br> - ê¸ì •/ë¶€ì • + ì£¼ê´€ì„± ì ìˆ˜ í•¨ê»˜ ì œê³µ | - ê°ì„± ì–´íœ˜ ì‚¬ì „ì´ ì˜ì–´ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì„±ë¨ <br> - ë¹„ê²©ì‹ì  ë¬¸ì¥(ì˜ˆ: SNS ì–¸ì–´)ì— ëŒ€í•œ ì •í™•ë„ ë‚®ìŒ <br> - ë¬¸ë§¥ íŒŒì•…ì´ ì•½í•¨ (ë‹¨ì–´ ë‹¨ìœ„ë¡œë§Œ íŒë‹¨) | - ê¸°ë³¸ì ì¸ ê°ì • ë¶„ì„ ì‹¤ìŠµ <br> - ë‰´ìŠ¤/ë¦¬ë·°ì˜ ì „ë°˜ì  ê°ì • ê²½í–¥ íŒŒì•… <br> - í•™ìŠµìš© or í”„ë¡œí† íƒ€ì… ë‹¨ê³„               |\n",
    "| **AFINN**                                                   | ë‹¨ì–´ë³„ ê°ì • ì ìˆ˜(-5~+5)ë¥¼ ì‚¬ì „ì— ì €ì¥í•´ í•©ì‚°í•˜ëŠ” ë‹¨ìˆœí•œ ê·œì¹™ ê¸°ë°˜ | - ì†ë„ê°€ ë§¤ìš° ë¹ ë¦„ <br> - ê²°ê³¼ê°€ ì§ê´€ì  (ë‹¨ì–´ ì ìˆ˜ì˜ í•©ìœ¼ë¡œ ê³„ì‚°) <br> - ê°€ë²¼ìš´ ë¶„ì„ì— ì í•©                                              | - ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì§€ ì•ŠìŒ (ë¶€ì •ì–´, ë°˜ì–´ë¬¸ ì²˜ë¦¬ ë¶ˆê°€) <br> - ê°ì • ë‹¨ì–´ ì‚¬ì „ì´ ì œí•œì ì„ <br> - ë¬¸ì¥ êµ¬ì¡°ë‚˜ í’ˆì‚¬ ê³ ë ¤ ë¶ˆê°€                | - íŠ¸ìœ—, ëŒ“ê¸€ ë“± ì§§ì€ ë¬¸ì¥ ê°ì„± ìŠ¤ì½”ì–´ë§ <br> - ì‹¤ì‹œê°„ ê°ì • ì§€ìˆ˜ ê³„ì‚°(ê°„ë‹¨í•œ ë¶„ì„)                            |\n",
    "| **VADER (Valence Aware Dictionary and sEntiment Reasoner)** | SNSÂ·ë¦¬ë·° ë°ì´í„°ìš©ìœ¼ë¡œ ì„¤ê³„ëœ ê°ì„±ì‚¬ì „ ê¸°ë°˜ ëª¨ë¸             | - ê°íƒ„ì‚¬, ëŒ€ë¬¸ì, ì´ëª¨í‹°ì½˜ ë“± ë¹„ì •í˜• ì–¸ì–´ ì¸ì‹ <br> - ë¶€ì •ì–´(NOT, never ë“±) ë¬¸ë§¥ ë°˜ì˜ ê°€ëŠ¥ <br> - ì†Œì…œë¯¸ë””ì–´ í…ìŠ¤íŠ¸ì— ë†’ì€ ì •í™•ë„                   | - ì˜ì–´ ì „ìš© <br> - ê¸´ ë¬¸ì¥ì´ë‚˜ ë³µì¡í•œ ë¬¸ë§¥ í•´ì„ í•œê³„ <br> - ì‚¬ìš©ì ì •ì˜ ì–´íœ˜ ì¶”ê°€ëŠ” ë‹¤ì†Œ ë²ˆê±°ë¡œì›€                            | - íŠ¸ìœ„í„°, ìœ íŠœë¸Œ ëŒ“ê¸€ ë“± SNS í…ìŠ¤íŠ¸ ë¶„ì„ <br> - ë¦¬ë·° ê°ì„± ë¶„ì„ (ê¸/ë¶€ì • ë¹„ìœ¨ ê³„ì‚°) <br> - ì‹¤ì‹œê°„ ì—¬ë¡  ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë“± |\n",
    "\n",
    "\n",
    "#### ğŸ“Œ ìš”ì•½ ì •ë¦¬\n",
    "- TextBlob â†’ ê°ì • + ë¬¸ë²• ë“± ê¸°ë³¸ NLP ì—°ìŠµìš©\n",
    "- AFINN â†’ ë‹¨ìˆœí•œ ì ìˆ˜ ê¸°ë°˜ ê°ì„± ì¸¡ì •ìš©\n",
    "- VADER â†’ SNSÂ·ë¦¬ë·° ë°ì´í„°ì— íŠ¹í™”ëœ ì‹¤ì œ ë¶„ì„ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f9497cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from textblob import TextBlob\n",
    "from afinn import Afinn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "#nltk ë‹¤ìš´ë¡œë“œ\n",
    "nltk.download('movie_reviews', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# ì˜í™” ë¦¬ë·° ë°ì´í„° ë¡œë“œ\n",
    "fileids = movie_reviews.fileids()\n",
    "reviews = [movie_reviews.raw(fileid) for fileid in fileids[:500]] + \\\n",
    "            [movie_reviews.raw(fileid) for fileid in fileids[-500:]] # ì•ì—ì„œ 500ê°œ ë’¤ì—ì„œ 500ê°œ\n",
    "categories = [movie_reviews.categories(fileid)[0] for fileid in fileids[:500]] +\\\n",
    "                [movie_reviews.categories(fileid)[0] for fileid in fileids[-500:]]  # ì•ì—ì„œ 500ê°œ ë’¤ì—ì„œ 500ê°œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92110fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 500, 500)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews) , categories.count('pos') , categories.count('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5282bc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •í™•ë„ :  0.6\n"
     ]
    }
   ],
   "source": [
    "# 1. TextBlob ì‚¬ìš©\n",
    "\n",
    "def sentiment_testblob(docs):\n",
    "    return [ 'pos' if TextBlob(doc).sentiment.polarity > 0 else 'neg' for doc in docs]\n",
    "predictions_texblob = sentiment_testblob(reviews)\n",
    "accuracy_textblob = accuracy_score(categories,predictions_texblob)\n",
    "print(f'ì •í™•ë„ : {accuracy_textblob : .1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56e34e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •í™•ë„ :  0.7\n"
     ]
    }
   ],
   "source": [
    "# 2. AFINN ì‚¬ìš©\n",
    "\n",
    "def sentiment_afinn(docs):\n",
    "    afn = Afinn(emoticons=True)\n",
    "    return [ 'pos' if afn.score(doc) > 0 else 'neg' for doc in docs ]\n",
    "predictions = sentiment_afinn(reviews)\n",
    "accuracy = accuracy_score(categories , predictions)\n",
    "print(f'ì •í™•ë„ : {accuracy : .1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "876192d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •í™•ë„ :  0.6\n"
     ]
    }
   ],
   "source": [
    "# 3. vadar\n",
    "\n",
    "def sentiment_vader(docs):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return [ 'pos' if analyzer.polarity_scores(doc)['compound'] > 0 else 'neg' for doc in docs ]\n",
    "predictions = sentiment_vader(reviews)\n",
    "accuracy = accuracy_score(categories , predictions)\n",
    "print(f'ì •í™•ë„ : {accuracy : .1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba746c",
   "metadata": {},
   "source": [
    "## - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215737b",
   "metadata": {},
   "source": [
    "## ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ\n",
    "\n",
    "### \"ì¢‹ë‹¤\" ë‹¨ì–´ë¥¼ ë³¸ í›„ ì´ ë¦¬ë·°ê°€ ê¸ì •ì¼ í™•ë¥ \n",
    "\n",
    "- ìš°ë¦¬ê°€ ì•Œê³  ì‹¶ì€ ê²ƒ:  \n",
    "  - **P(ê¸ì • | \"ì¢‹ë‹¤\")** = \"ë¦¬ë·°ì— 'ì¢‹ë‹¤'ê°€ ìˆì„ ë•Œ, ì´ ë¦¬ë·°ê°€ ê¸ì •ì¼ í™•ë¥ \"\n",
    "\n",
    "- ë² ì´ì¦ˆ ê³µì‹ ì ìš©:\n",
    "  - **P(ê¸ì • | \"ì¢‹ë‹¤\") = P(\"ì¢‹ë‹¤\" | ê¸ì •) Ã— P(ê¸ì •) / P(\"ì¢‹ë‹¤\")**\n",
    "\n",
    "- ê° í•­ì˜ ì˜ë¯¸:\n",
    "  - **P(\"ì¢‹ë‹¤\" | ê¸ì •)** : ê¸ì • ë¦¬ë·°ë“¤ ì¤‘ì—ì„œ \"ì¢‹ë‹¤\"ê°€ ë‚˜ì˜¬ í™•ë¥   \n",
    "  - **P(ê¸ì •)** : ì „ì²´ ë¦¬ë·° ì¤‘ì—ì„œ ê¸ì • ë¦¬ë·° ë¹„ìœ¨  \n",
    "  - **P(\"ì¢‹ë‹¤\")** : ì „ì²´ ë¦¬ë·°ì—ì„œ \"ì¢‹ë‹¤\" ë‹¨ì–´ê°€ ë“±ì¥í•  í™•ë¥   \n",
    "\n",
    "\n",
    "| êµ¬ë¶„ | ë‚´ìš© |\n",
    "|:-----|:-----|\n",
    "| **ì£¼ìš” ìš©ë„** | - ì´ë©”ì¼ **ìŠ¤íŒ¸ vs ì •ìƒ** ë¶„ë¥˜<br> - ì˜í™”/ìƒí’ˆ ë¦¬ë·° **ê¸ì • vs ë¶€ì • ê°ì„± ë¶„ì„**<br> - ë‰´ìŠ¤/ë¬¸ì„œ **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜** (ì •ì¹˜, ê²½ì œ, ìŠ¤í¬ì¸  ë“±)<br> - ê³ ì°¨ì› í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¹ ë¥¸ ë¶„ë¥˜ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ |\n",
    "| **ì¥ì ** | - ê³„ì‚°ì´ ë§¤ìš° ë¹ ë¥´ê³  êµ¬í˜„ì´ ê°„ë‹¨í•¨<br> - ì ì€ ë°ì´í„°ë¡œë„ ì–´ëŠ ì •ë„ ì„±ëŠ¥ì´ ë‚˜ì˜´<br> - ê³ ì°¨ì›(ë‹¨ì–´ ìˆ˜ê°€ ë§ì€) ë°ì´í„°ì— ê°•í•¨<br> - ê³¼ì í•© ìœ„í—˜ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ê³  í•´ì„ì´ ì§ê´€ì  |\n",
    "| **ë‹¨ì ** | - íŠ¹ì„±(ë‹¨ì–´)ë“¤ì´ ì„œë¡œ **ë…ë¦½**ì´ë¼ê³  ê°€ì • â†’ í˜„ì‹¤ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ<br> - ë¬¸ë§¥, ë°˜ì–´ë²•, ë³µí•© í‘œí˜„ì— ì•½í•¨ (ë¯¸ë¬˜í•œ ê°ì • í‘œí˜„ í•´ì„ í•œê³„)<br> - íŠ¹ì • ë‹¨ì–´ê°€ í•œ í´ë˜ìŠ¤ì—ë§Œ ë§ì´ ìˆì„ ê²½ìš° í™•ë¥ ì´ ê·¹ë‹¨ì ìœ¼ë¡œ ì¹˜ìš°ì¹  ìˆ˜ ìˆìŒ |\n",
    "\n",
    "#### ğŸ“Œ ìš”ì•½ ì •ë¦¬\n",
    "- ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ â†’ â€œë¹ ë¥´ê³  ì‹¤ìš©ì ì¸ ë¶„ë¥˜ ëª¨ë¸â€ (ë¨¸ì‹ ëŸ¬ë‹ ì ‘ê·¼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ca945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ê°ì„±ë¶„ì„\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5cf3f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 800)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„° ë¶„í• \n",
    "dataset = train_test_split(reviews, categories, test_size=0.2,random_state=42, stratify=categories)\n",
    "len(dataset[0]),len(dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1576e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf ë²¡í„°í™”\n",
    "vectorizer =  TfidfVectorizer(max_features=1000)\n",
    "x_train = vectorizer.fit_transform(dataset[0])\n",
    "x_test = vectorizer.transform(dataset[1])\n",
    "\n",
    "y_train = dataset[2]\n",
    "y_test = dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66942dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.77      0.82      0.79       100\n",
      "         pos       0.81      0.75      0.78       100\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.79      0.78      0.78       200\n",
      "weighted avg       0.79      0.79      0.78       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Mnb\n",
    "mnb_clf = MultinomialNB()\n",
    "mnb_clf.fit(x_train,y_train)\n",
    "predict = mnb_clf.predict(x_test)\n",
    "print( classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef32b6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.79      0.79      0.79       100\n",
      "         pos       0.79      0.79      0.79       100\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.79      0.79      0.79       200\n",
      "weighted avg       0.79      0.79      0.79       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. LogisticRegression\n",
    "logi_clf = LogisticRegression()\n",
    "logi_clf.fit(x_train, y_train)\n",
    "predict = logi_clf.predict(x_test)\n",
    "print(classification_report(y_test , predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ca35331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.86      0.85       100\n",
      "         pos       0.86      0.83      0.84       100\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.85      0.84      0.84       200\n",
      "weighted avg       0.85      0.84      0.84       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ì„±ëŠ¥í–¥ìƒ\n",
    "# ì†Œë¬¸ìë³€í™˜ - ì—°ì†ëœ ë¬¸ìì—´ì¤‘ì— 3ê¸€ì ì´ìƒ - ì–´ê°„ì¶”ì¶œ(í˜•íƒœì†Œë¶„ì„) - ë¶ˆìš©ì–´ ì œê±°\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer =  RegexpTokenizer(r\"[\\w']{3,}\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    porter = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [porter.stem(token) for token in tokens if token not in stop_words]\n",
    "vector = TfidfVectorizer(\n",
    "    tokenizer  = custom_tokenizer\n",
    "    ,max_features=1000\n",
    "    ,min_df=5\n",
    "    ,max_df=0.5\n",
    "    ,token_pattern = r\"[\\w']{3,}\"\n",
    ")\n",
    "x_train = vector.fit_transform(dataset[0])\n",
    "x_test = vector.transform(dataset[1])\n",
    "\n",
    "def evaluate_model(model):    \n",
    "    model.fit(x_train,y_train)\n",
    "    predict = model.predict(x_test)\n",
    "    print( classification_report(y_test, predict))\n",
    "    \n",
    "evaluate_model(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81932c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
